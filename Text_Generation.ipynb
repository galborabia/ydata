{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inbarhub/YDATA_DL_assignments_2021-2022/blob/main/H.W_9_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88X5OhoAJEJh"
      },
      "source": [
        "# RNN for text generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh_aPOY6JEJz"
      },
      "source": [
        "In this exercise, you'll unleash the hidden creativity of your computer, by letting it generate Country songs (yeehaw!). You'll train a character-level RNN-based language model, and use it to generate new songs.\n",
        "\n",
        "\n",
        "### Special Note\n",
        "\n",
        "Our Deep Learning course was packed with both theory and practice. In a short time, you've got to learn the basics of deep learning theory and get hands-on experience training and using pretrained DL networks, while learning PyTorch.  \n",
        "Past exercises required a lot of work, and hopefully gave you a sense of the challenges and difficulties one faces when using deep learning in the real world. While the investment you've made in the course so far is enormous, We strongly encourage you to take a stab at this exercise. \n",
        "\n",
        "Some songs contain no lyrics (for example, they just contain the text \"instrumental\"). Others include non-English characters. You'll often need to preprocess your data and make decisions as to what your network should actually get as input (think - how should you treat newline characters?)\n",
        "\n",
        "More issues will probably pop up while you're working on this task. If you face technical difficulties or find a step in the process that takes too long, please let me know. It would also be great if you share with the class code you wrote that speeds up some of the work (for example, a data loader class, a parsed dataset etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjwCeQRJEJ_"
      },
      "source": [
        "## RNN for Text Generation\n",
        "In this section, we'll use an LSTM to generate new songs. You can pick any genre you like, or just use all genres. You can even try to generate songs in the style of a certain artist - remember that the Metrolyrics dataset contains the author of each song. \n",
        "\n",
        "For this, we’ll first train a character-based language model. We’ve mostly discussed in class the usage of RNNs to predict the next word given past words, but as we’ve mentioned in class, RNNs can also be used to learn sequences of characters.\n",
        "\n",
        "First, please go through the [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) on generating family names. You can download a .py file or a jupyter notebook with the entire code of the tutorial. \n",
        "\n",
        "As a reminder of topics we've discussed in class, see Andrej Karpathy's popular blog post [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). You are also encouraged to view [this](https://gist.github.com/karpathy/d4dee566867f8291f086) vanilla implementation of a character-level RNN, written in numpy with just 100 lines of code, including the forward and backward passes.  \n",
        "\n",
        "Other tutorials that might prove useful:\n",
        "1. http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/\n",
        "1. https://github.com/mcleonard/pytorch-charRNN\n",
        "1. https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from torch.utils.data.distributed import Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "B1iLmS2l2wVg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data preprocessing"
      ],
      "metadata": {
        "id": "BZAB-Pv92GUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset\n",
        "parquet_file = 'https://github.com/omriallouche/ydata_deep_learning_2021/blob/master/data/metrolyrics.parquet?raw=true'\n",
        "data = pd.read_parquet(parquet_file, engine='auto')\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0rEnCyDQ2XOE",
        "outputId": "72edb5b1-e831-4139-bfc6-bcbf7f0e59de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     song  year           artist    genre  \\\n",
              "204182                      fully-dressed  2008            annie      Pop   \n",
              "6116                   surrounded-by-hoes  2006          50-cent  Hip-Hop   \n",
              "166369  taste-the-tears-thunderpuss-remix  2006            amber      Pop   \n",
              "198416         the-truth-will-set-me-free  2006     glenn-hughes     Rock   \n",
              "127800                   the-last-goodbye  2008  aaron-pritchett  Country   \n",
              "...                                   ...   ...              ...      ...   \n",
              "33205             give-it-all-up-for-love  2007       bananarama      Pop   \n",
              "194149      all-i-m-thinking-about-is-you  2000  billy-ray-cyrus     Rock   \n",
              "11649                   bonsoir-mon-amour  2015           dalida      Pop   \n",
              "252283             i-m-not-gonna-miss-you  2014    glen-campbell      Pop   \n",
              "11180                        i-am-the-man  2008        the-czars     Rock   \n",
              "\n",
              "                                                   lyrics  num_chars  \\\n",
              "204182  [HEALY]\\n[spoken] This is Bert Healy saying .....       1041   \n",
              "6116    [Chorus: repeat 2X] Even when I'm tryin to be ...       1392   \n",
              "166369  How could you cause me so much pain?\\nAnd leav...       1113   \n",
              "198416  In a scarlet vision\\nIn a velvet room\\nI come ...        779   \n",
              "127800  Sprintime in Savannah\\nIt dont get much pretti...        881   \n",
              "...                                                   ...        ...   \n",
              "33205   To all the men I knew before\\nOld love letters...       1159   \n",
              "194149  Well it's a twenty-five mile drive from here t...       1094   \n",
              "11649   Tu viens de partir pour de longs mois, c'est l...        455   \n",
              "252283  I'm still here, but yet I'm gone\\nI don't play...        527   \n",
              "11180   You are beyond any reproach now\\nYou are so co...        715   \n",
              "\n",
              "                                                     sent  num_words  \n",
              "204182  healy spoken this bert healy saying singing he...        826  \n",
              "6116    chorus repeat x even i tryin low i recognized ...        884  \n",
              "166369  how could cause much pain and leave heart rain...        756  \n",
              "198416  in scarlet vision in velvet room i come decisi...        583  \n",
              "127800  sprintime savannah it dont get much prettier b...        639  \n",
              "...                                                   ...        ...  \n",
              "33205   to men i knew old love letters drawer mean not...        712  \n",
              "194149  well twenty five mile drive town ther gray ski...        676  \n",
              "11649   tu viens de partir pour de longs mois c est lo...        426  \n",
              "252283  i still yet i gone i play guitar sing songs th...        344  \n",
              "11180   you beyond reproach you cool youre flawless pe...        446  \n",
              "\n",
              "[49976 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a426ac5e-cc68-4b16-81d8-b431a6dae8d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song</th>\n",
              "      <th>year</th>\n",
              "      <th>artist</th>\n",
              "      <th>genre</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>sent</th>\n",
              "      <th>num_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>204182</th>\n",
              "      <td>fully-dressed</td>\n",
              "      <td>2008</td>\n",
              "      <td>annie</td>\n",
              "      <td>Pop</td>\n",
              "      <td>[HEALY]\\n[spoken] This is Bert Healy saying .....</td>\n",
              "      <td>1041</td>\n",
              "      <td>healy spoken this bert healy saying singing he...</td>\n",
              "      <td>826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6116</th>\n",
              "      <td>surrounded-by-hoes</td>\n",
              "      <td>2006</td>\n",
              "      <td>50-cent</td>\n",
              "      <td>Hip-Hop</td>\n",
              "      <td>[Chorus: repeat 2X] Even when I'm tryin to be ...</td>\n",
              "      <td>1392</td>\n",
              "      <td>chorus repeat x even i tryin low i recognized ...</td>\n",
              "      <td>884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166369</th>\n",
              "      <td>taste-the-tears-thunderpuss-remix</td>\n",
              "      <td>2006</td>\n",
              "      <td>amber</td>\n",
              "      <td>Pop</td>\n",
              "      <td>How could you cause me so much pain?\\nAnd leav...</td>\n",
              "      <td>1113</td>\n",
              "      <td>how could cause much pain and leave heart rain...</td>\n",
              "      <td>756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198416</th>\n",
              "      <td>the-truth-will-set-me-free</td>\n",
              "      <td>2006</td>\n",
              "      <td>glenn-hughes</td>\n",
              "      <td>Rock</td>\n",
              "      <td>In a scarlet vision\\nIn a velvet room\\nI come ...</td>\n",
              "      <td>779</td>\n",
              "      <td>in scarlet vision in velvet room i come decisi...</td>\n",
              "      <td>583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127800</th>\n",
              "      <td>the-last-goodbye</td>\n",
              "      <td>2008</td>\n",
              "      <td>aaron-pritchett</td>\n",
              "      <td>Country</td>\n",
              "      <td>Sprintime in Savannah\\nIt dont get much pretti...</td>\n",
              "      <td>881</td>\n",
              "      <td>sprintime savannah it dont get much prettier b...</td>\n",
              "      <td>639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33205</th>\n",
              "      <td>give-it-all-up-for-love</td>\n",
              "      <td>2007</td>\n",
              "      <td>bananarama</td>\n",
              "      <td>Pop</td>\n",
              "      <td>To all the men I knew before\\nOld love letters...</td>\n",
              "      <td>1159</td>\n",
              "      <td>to men i knew old love letters drawer mean not...</td>\n",
              "      <td>712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194149</th>\n",
              "      <td>all-i-m-thinking-about-is-you</td>\n",
              "      <td>2000</td>\n",
              "      <td>billy-ray-cyrus</td>\n",
              "      <td>Rock</td>\n",
              "      <td>Well it's a twenty-five mile drive from here t...</td>\n",
              "      <td>1094</td>\n",
              "      <td>well twenty five mile drive town ther gray ski...</td>\n",
              "      <td>676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11649</th>\n",
              "      <td>bonsoir-mon-amour</td>\n",
              "      <td>2015</td>\n",
              "      <td>dalida</td>\n",
              "      <td>Pop</td>\n",
              "      <td>Tu viens de partir pour de longs mois, c'est l...</td>\n",
              "      <td>455</td>\n",
              "      <td>tu viens de partir pour de longs mois c est lo...</td>\n",
              "      <td>426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252283</th>\n",
              "      <td>i-m-not-gonna-miss-you</td>\n",
              "      <td>2014</td>\n",
              "      <td>glen-campbell</td>\n",
              "      <td>Pop</td>\n",
              "      <td>I'm still here, but yet I'm gone\\nI don't play...</td>\n",
              "      <td>527</td>\n",
              "      <td>i still yet i gone i play guitar sing songs th...</td>\n",
              "      <td>344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11180</th>\n",
              "      <td>i-am-the-man</td>\n",
              "      <td>2008</td>\n",
              "      <td>the-czars</td>\n",
              "      <td>Rock</td>\n",
              "      <td>You are beyond any reproach now\\nYou are so co...</td>\n",
              "      <td>715</td>\n",
              "      <td>you beyond reproach you cool youre flawless pe...</td>\n",
              "      <td>446</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49976 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a426ac5e-cc68-4b16-81d8-b431a6dae8d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a426ac5e-cc68-4b16-81d8-b431a6dae8d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a426ac5e-cc68-4b16-81d8-b431a6dae8d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clean text"
      ],
      "metadata": {
        "id": "psn8tgE_wBJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def song_processing(song):\n",
        "  # remove words that are between []\n",
        "  song = re.sub( r\"\\[([^\\]]+)\\]\", '', song)\n",
        "  song = song.lower()\n",
        "  # remove none english characters\n",
        "  song = re.sub(r\"[^a-zA-Z,.'!\\n]\", ' ', song)\n",
        "  song = song.strip()\n",
        "  return song\n",
        "\n",
        "data['clean_song'] = data['lyrics'].apply(lambda x: song_processing(x))\n",
        "# remove empty songs\n",
        "data = data[data['clean_song'].str.len() > 0]\n",
        "data['clean_song']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHJDMyiS3dnT",
        "outputId": "bffc0cf3-dd45-4aa7-e7f7-854530ddc8da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204182    this is bert healy saying ...\\n hey, hobo man\\...\n",
              "6116      even when i'm tryin to be on the low, i'm reco...\n",
              "166369    how could you cause me so much pain \\nand leav...\n",
              "198416    in a scarlet vision\\nin a velvet room\\ni come ...\n",
              "127800    sprintime in savannah\\nit dont get much pretti...\n",
              "                                ...                        \n",
              "33205     to all the men i knew before\\nold love letters...\n",
              "194149    well it's a twenty five mile drive from here t...\n",
              "11649     tu viens de partir pour de longs mois, c'est l...\n",
              "252283    i'm still here, but yet i'm gone\\ni don't play...\n",
              "11180     you are beyond any reproach now\\nyou are so co...\n",
              "Name: clean_song, Length: 49971, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### parameters"
      ],
      "metadata": {
        "id": "ZkoHpDo2wGmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EMBEDDING_SIZE = 100\n",
        "SEQUENCE_LENGTH = 1023\n",
        "MAX_LENGTH = 1024"
      ],
      "metadata": {
        "id": "b7U4rpmUZix7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Char tokenizer"
      ],
      "metadata": {
        "id": "hNc_efnowhLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharTokenizer:\n",
        "\n",
        "    def __init__(self, char_to_int, max_length=1024, padding_token='@'):\n",
        "      self.char_to_int = char_to_int\n",
        "      self.int_to_char = {v: k for k, v in char_to_int.items()}\n",
        "      self.padding_token = padding_token\n",
        "      self.max_length = max_length\n",
        "\n",
        "    def _pad(self, encoded_text):\n",
        "\n",
        "      num_elements_to_add = self.max_length - encoded_text.size(0)\n",
        "      padding_value = self.char_to_int[self.padding_token]\n",
        "      padding_tensor = torch.empty(num_elements_to_add).fill_(padding_value)\n",
        "      return torch.cat((encoded_text, padding_tensor))\n",
        "\n",
        "\n",
        "    def encode(self, text, do_padding=True):\n",
        "      encoded_text = [self.char_to_int[char] for char in text]\n",
        "      encoded_text = torch.tensor(encoded_text)\n",
        "      if do_padding:\n",
        "        if len(encoded_text) >= self.max_length:\n",
        "          return encoded_text[:self.max_length]\n",
        "        return self._pad(encoded_text)\n",
        "      else:\n",
        "        return encoded_text\n",
        "    \n",
        "    def decode(self, tokens):\n",
        "      return ''.join([self.int_to_char[token] for token in tokens])\n",
        "\n",
        "\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz .,'!\\n\"\n",
        "\n",
        "# padding character \n",
        "char_to_int = {'@': 0}\n",
        "\n",
        "for i, char in enumerate(chars):\n",
        "    char_to_int[char] = i + 1\n",
        "\n",
        "songs_df = data[data['clean_song'].str.len() >= MAX_LENGTH]\n",
        "\n",
        "tokenizer = CharTokenizer(char_to_int, max_length=MAX_LENGTH)\n",
        "encoded_songs = songs_df['clean_song'].apply(lambda x: tokenizer.encode(x).numpy())\n",
        "\n",
        "number_of_songs = len(encoded_songs)\n",
        "encoded_songs = torch.tensor(np.concatenate(encoded_songs.values).reshape(number_of_songs, MAX_LENGTH))\n",
        "\n",
        "# split to train test\n",
        "train, test = train_test_split(encoded_songs, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "rgNGR1wXE46u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Song dataset"
      ],
      "metadata": {
        "id": "Z_fovufvwtHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SongDataset(Dataset):\n",
        "  def __init__(self, songs, seq_length=128):\n",
        "    self.songs = songs\n",
        "    self.seq_length = seq_length\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.songs)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    song = self.songs[idx]\n",
        "    input_seq = song[:self.seq_length].reshape(-1,1).int()\n",
        "    target_seq = song[self.seq_length].long()\n",
        "    return input_seq, target_seq\n",
        "  \n",
        "train_dataset = SongDataset(train, seq_length=SEQUENCE_LENGTH)\n",
        "validation_dataset = SongDataset(test, seq_length=SEQUENCE_LENGTH)\n",
        "\n",
        "songs_datasets = {'train': train_dataset,\n",
        "                  'val': validation_dataset}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(songs_datasets['train'], batch_size=BATCH_SIZE,\n",
        "                                             shuffle=False, num_workers=2),\n",
        "               \n",
        "    'val': torch.utils.data.DataLoader(songs_datasets['val'], batch_size=BATCH_SIZE,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "  }\n",
        "\n",
        "dataset_sizes = {x: len(songs_datasets[x]) for x in ['train', 'val']}\n",
        "print('dataset_sizes: ', dataset_sizes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx7JffQPw4pa",
        "outputId": "5388ae94-60c6-450c-c454-104d6802fe52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_sizes:  {'train': 23037, 'val': 2560}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "SrDXceqAwxnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training function"
      ],
      "metadata": {
        "id": "VR8V8Eu0w5JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Init variables that will save info about the best model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "\n",
        "    train_res= np.zeros((2,num_epochs))\n",
        "    val_res=np.zeros((2,num_epochs))\n",
        "    dict_res={'train':train_res, 'val':val_res}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                # Set model to training mode. \n",
        "                model.train()  \n",
        "            else:\n",
        "                # Set model to evaluate mode. In evaluate mode, we don't perform backprop and don't need to keep the gradients\n",
        "                model.eval()   \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # Prepare the inputs for GPU/CPU\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # ===== forward pass ======\n",
        "                with torch.set_grad_enabled(phase=='train'):\n",
        "                    # If we're in train mode, we'll track the gradients to allow back-propagation\n",
        "                    outputs, hidden = model(inputs, hidden)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # ==== backward pass + optimizer step ====\n",
        "                    # This runs only in the training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() # Perform a step in the opposite direction of the gradient\n",
        "                        optimizer.step() # Adapt the optimizer\n",
        "                        hidden = tuple([h.detach() for h in hidden])\n",
        "\n",
        "                # Collect statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                # Adjust the learning rate based on the scheduler\n",
        "                scheduler.step()  \n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            dict_res[phase][0,epoch]=epoch_loss\n",
        "\n",
        "            # Keep the results of the best model so far\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                # deepcopy the model\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s')\n",
        "    print(f'Best loss {round(best_loss, 4)}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, dict_res"
      ],
      "metadata": {
        "id": "9NUVMtq-z1BU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "R8KJ2yGbw9tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharModel(nn.Module):\n",
        "    def __init__(self, number_of_outputs, embedding_size=64, hidden_size=128, num_layers=2, dropout_probability=0.1):\n",
        "        super(CharModel, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(number_of_outputs, embedding_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_probability)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.linear = nn.Linear(hidden_size, number_of_outputs)\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
        "                   weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
        "\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(x.shape[0], x.shape[1], x.shape[3])\n",
        "        x, hidden = self.lstm(x)\n",
        "        \n",
        "        # take only the last output\n",
        "        x = x[:, -1, :]\n",
        "       \n",
        "        # produce output\n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x, hidden\n",
        "\n",
        "number_of_outputs = len(tokenizer.char_to_int)\n",
        "model = CharModel(number_of_outputs, embedding_size=EMBEDDING_SIZE, hidden_size=512, num_layers=2)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK2-WiqObHNY",
        "outputId": "08204211-4ab7-4c8f-b225-d5016e62f748"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (embedding): Embedding(33, 100, padding_idx=0)\n",
              "  (lstm): LSTM(100, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (linear): Linear(in_features=512, out_features=33, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If a GPU is available, make the model use it\n",
        "torch.cuda.empty_cache()\n",
        "model = model.to(device)\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.Adam(model.parameters(), lr=lr, weight_decay=4e-4)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.2)\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "model,dict_res = train_model(model.to(device), \n",
        "                    dataloaders,\n",
        "                       criterion, \n",
        "                       optimizer_ft, \n",
        "                       exp_lr_scheduler,\n",
        "                       num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdl1XauC0Brk",
        "outputId": "2d34961e-96a3-4d77-a4ca-9adbe7d173e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 2.4773 Acc: 0.2954\n",
            "val Loss: 2.2669 Acc: 0.3348\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 2.1736 Acc: 0.3632\n",
            "val Loss: 2.1577 Acc: 0.3621\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 2.0747 Acc: 0.3863\n",
            "val Loss: 2.1003 Acc: 0.3832\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 2.0135 Acc: 0.4020\n",
            "val Loss: 2.0643 Acc: 0.3879\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 1.9599 Acc: 0.4116\n",
            "val Loss: 2.0252 Acc: 0.3914\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 1.9117 Acc: 0.4221\n",
            "val Loss: 1.9982 Acc: 0.4066\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 1.8682 Acc: 0.4354\n",
            "val Loss: 1.9764 Acc: 0.4156\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 1.8227 Acc: 0.4479\n",
            "val Loss: 1.9742 Acc: 0.4133\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 1.7757 Acc: 0.4610\n",
            "val Loss: 1.9716 Acc: 0.4145\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 1.7306 Acc: 0.4748\n",
            "val Loss: 1.9820 Acc: 0.4160\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 1.5775 Acc: 0.5176\n",
            "val Loss: 1.9497 Acc: 0.4289\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 1.5018 Acc: 0.5395\n",
            "val Loss: 1.9502 Acc: 0.4383\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 1.4512 Acc: 0.5549\n",
            "val Loss: 1.9519 Acc: 0.4348\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 1.4020 Acc: 0.5686\n",
            "val Loss: 1.9688 Acc: 0.4340\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 1.3435 Acc: 0.5900\n",
            "val Loss: 1.9865 Acc: 0.4332\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 1.3084 Acc: 0.5957\n",
            "val Loss: 2.0102 Acc: 0.4359\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 1.2411 Acc: 0.6191\n",
            "val Loss: 2.0442 Acc: 0.4309\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 1.1809 Acc: 0.6374\n",
            "val Loss: 2.0756 Acc: 0.4242\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 1.1188 Acc: 0.6615\n",
            "val Loss: 2.1075 Acc: 0.4273\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 1.0518 Acc: 0.6790\n",
            "val Loss: 2.1487 Acc: 0.4191\n",
            "\n",
            "Training complete in 29m 19s\n",
            "Best loss 1.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generated songs"
      ],
      "metadata": {
        "id": "kyA2PG5JxIxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_song(start_sequence, model, max_length=1024):\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      current_seq = start_sequence\n",
        "      hidden = model.init_hidden(1)\n",
        "      \n",
        "      # Generate text character by character\n",
        "      for _ in range(max_length):\n",
        "          input_tensor = tokenizer.encode(current_seq, do_padding=False).reshape((1, -1, 1)).int().to(device)\n",
        "          output, hidden = model(input_tensor, hidden)\n",
        "          probabilities = F.softmax(output, dim=1).squeeze()\n",
        "          \n",
        "          # Sample the next character based on the probabilities\n",
        "          next_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "          next_char = tokenizer.decode([next_id])\n",
        "          # Append the sampled character to the generated sequence\n",
        "          current_seq += next_char\n",
        "\n",
        "  return current_seq\n",
        "        \n"
      ],
      "metadata": {
        "id": "OUlO9DCYhsuG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = tokenizer.decode(test[0][0:128].int().numpy())\n",
        "\n",
        "print(f\"Start sequence: \\n{start_sequence}\")\n",
        "generated_song = generate_song(start_sequence, model)\n",
        "print()\n",
        "print(f\"Generated song: \\n{generated_song}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tro7807Np6lF",
        "outputId": "716260d8-3477-447f-d6da-86e4c4566c9f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start sequence: \n",
            "get up out your seat, the grand groove is back\n",
            "with a beat to sink your teeth in like wolfman jack\n",
            "i'm bringing in the swing lik\n",
            "\n",
            "Generated song: \n",
            "get up out your seat, the grand groove is back\n",
            "with a beat to sink your teeth in like wolfman jack\n",
            "i'm bringing in the swing like an awand\n",
            "and it rast g\n",
            "tere the clist the s at matchiri's arouin\n",
            " heap that money, in trying, the ray\n",
            "be voyy a stast a mot about\n",
            "steating for's for and let use toon bood\n",
            "oh,, rom take in i loved pryades the reand of a bed\n",
            "gend your niggat time and tirn that like youq pust crouss no warnin' one waind you get out we ushin' a wishig shits\n",
            "\n",
            "achin' don't leed me you don't got to every good tfee arving pall\n",
            "my, i wet you sping worry the  came no my life of the doy\n",
            "fhave out the moins leant, i'll neving eningin' free on the handy\n",
            "jon't got got baby bindin' up\n",
            "your misca, it's a lise\n",
            "four huf all packs t leoming my smowy  now lat then all i manded been mymarn, the rusterfer why ok you, dieny i'mn time ton tame is\n",
            "a minneana my don't tome\n",
            "but i dney me\n",
            "with thing what whil\n",
            "you can you sal i\n",
            "no not's a tare you cast on to more a worle's saf i so suver\n",
            "the mast wan grives. yo, your bany, eash ol the deft tame wock that clore no my skend, been. swer endy love you\n",
            "they for it me, around i know to she you not@ey deat yo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = tokenizer.decode(test[1][0:128].int().numpy())\n",
        "\n",
        "print(f\"Start sequence: \\n{start_sequence}\")\n",
        "generated_song = generate_song(start_sequence, model)\n",
        "print()\n",
        "print(f\"Generated song: \\n{generated_song}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGmReCJ6srr3",
        "outputId": "190ac842-9911-4b34-dafe-f7be06fb13a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start sequence: \n",
            "in my early years i hid my tears\n",
            "and passed my days alone\n",
            "adrift on an ocean of loneliness\n",
            "my dreams like nets were thrown\n",
            "to ca\n",
            "\n",
            "Generated song: \n",
            "in my early years i hid my tears\n",
            "and passed my days alone\n",
            "adrift on an ocean of loneliness\n",
            "my dreams like nets were thrown\n",
            "to care de the timine at tating nigga fast's spir that shill a caan love, i've alwag but in hur veen your the blocke\n",
            "thy way it you you wit, way to tome\n",
            "whyund you meand ang thesh i icm na fed you can know, move your loog and heard out shifhis ,y notgit vuscared up the lieving time, ourine\n",
            "but i more my hadn the dect\n",
            "ace teand thing jass\n",
            "but i loving dow bow dow now in a our sined you  aserof\n",
            "and your fut i deed eneer daby\n",
            "and reand, she@in's.ded ut proke me, dows to the dag a buf is hear, noto shatt!as\n",
            "in\n",
            "you're got it fleat ain's never gids\n",
            "i'm whon to ptart dear the myoce to dratear\n",
            "herus ous could bet down just awarn out \n",
            "runsy you're antta be wibde sweed your a is nothing you everywe unleared it pelieve oothing a do you tay beoply be ard goon\n",
            "you in doing the i'll wanna about in tern to talk rialmting at cam ternet la!e proud\n",
            "ef crkame, me mainy sife oo bust want you goin' 'pee yur arrust to\n",
            "spick thing stading\n",
            "youbre going noo semmit, al awhtring byow way   tigging haid out to im,\n",
            "in mead en a shele\n",
            "she don'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = tokenizer.decode(test[2][0:128].int().numpy())\n",
        "\n",
        "print(f\"Start sequence: \\n{start_sequence}\")\n",
        "generated_song = generate_song(start_sequence, model)\n",
        "print()\n",
        "print(f\"Generated song: \\n{generated_song}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE83PCqAsr41",
        "outputId": "d47a515c-24c8-45d2-8934-0b810e8edd78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start sequence: \n",
            "yeah! \n",
            " woo! \n",
            "dig, dig, gravedigger\n",
            "dig, gravedigger, dig\n",
            "work that shovel with vigor, gravedigger\n",
            "before rigor mortis sets in, \n",
            "\n",
            "Generated song: \n",
            "yeah! \n",
            " woo! \n",
            "dig, dig, gravedigger\n",
            "dig, gravedigger, dig\n",
            "work that shovel with vigor, gravedigger\n",
            "before rigor mortis sets in, norned you take you they been digga veaws woure not  now not in to all stand out heald the pain for lof me the tron\n",
            "care\n",
            "you chorgice if hap lo muor\n",
            "no in bood your're go and rould that niggas heid in the mand\n",
            "and here i'd lough wo know o busce\n",
            "thing is in shigh rouch than loo\n",
            "hild bust with me \n",
            "won't din't gift allin' up really all it know baw\n",
            "you, sitis take to of\n",
            "and af a miand fbleave\n",
            "eve im me pand on what rin' ho\n",
            "look anoh repmemame inny praugh\n",
            "and you, were ther werre\n",
            "kust about been it id you thin' this\n",
            "not\n",
            "i get it telvel the may or you fing togath\n",
            "thing reriseve liaking brack mach\n",
            "my cow like a doy\n",
            "in the came the stife it\n",
            "but we know haud talde and i mant\n",
            "te look in if miere called so mer, ace stang is fide, you're tien on here\n",
            "you car take the lare\n",
            "\n",
            "bery pust a will with i is we'lr we old you rtein' a ming and lest me know we've were giv tien but let me giving for for your fleas\n",
            "reah, ret me how the torcat know in when you know way you pying mame i deave to dicciraiy\n",
            "pear fight\n",
            "in here won't doog \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}